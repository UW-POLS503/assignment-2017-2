---
title: "Solutions to POLS 503 Assignment 2"
author: "Sebastian Mayer"
date: "05/03/2017"
output: html_document
---
```{r}
#library("git2r")
#config(global = TRUE, user.email = "youremail@whatever", user.name = "yourname")
```

# Setup

```{r message=FALSE}
library("tidyverse")
library("modelr")
```

While only a subset of the original data of @Yule1899a was printed in the article itself, @Plewis2015a reconstructed the orginal data and @Plewis2017a replicated the original paper. This data is included in the package **datums**. This package is not on CRAN, but can be downloaded from github.
**IMPORTANT** install the latest version of **datums** since a few fixes were recently made to the `pauperism` dataset.

```{r}
#install.packages("devtools")
# devtools::install_github("jrnold/datums")
library("datums")
```

The data for @Yule1899a is split into two data frames: `pauperism_plu` contains data on the Poor Law Unions (PLU), and `pauperism_year`, panel data with the PLU-year as the unit of observation.
```{r}
pauperism <-
  left_join(datums::pauperism_plu, datums::pauperism_year,
            by = "ID")
            
```



Fill in your solutions here

## Original Specification

Run regressions of `pauper` using the yearly level data with the following specifications. 

```{r}
M1 <- lm(paupratiodiff ~ outratiodiff + year + Type, data = pauperism)
M2 <- lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism)
M3 <- lm(-1  + paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year + Type), data = pauperism)
M4 <- lm(paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year + Type), data = pauperism)

M1
M2
M3
M4
```

1. Present the regressions results in a regression table


```{r}
library("stargazer")

stargazer(M1, M2, M3, M4, type = "text")
```

2. Interpret the coefficients for `outratiodiff` for each model.

- For M1: A one unit change in outratiodiff is associated on average with a 0.2343 difference in pauperism.
Hence, as proportion of people receiving outwelfare increases, pauperism increases as well. 

- For M2-M4: The magnitudes of the changes are different, but the way we interpret it stays the same. We just hold more things constant. 


3. Write the equations for each or all models, and describe the model with a sentence or two. Try to be as concise as possible. Look at recent journal articles for examples of the wording and format.

- M1: y = ß0 + ß1x1 + ß2x2 + ß3x3 + e

This model seeks to explain the relationship between outrelief and pauperism; it tests if there is a linear relationship of outrelief effecting pauperism, while holding year and type constant.

- M2: y = ß0 + ß1x1 + ß2x2 + ß3x3 + ßx4x2 + ßx4x3 + ßx5x2 + ßx5x3 + e

This model tests if outrelief has an influence on pauperism, but is dependent on the year/area (is conditioned on the year/area), while also holding the population ratio and the old ratio constant. 


- M3: y = -1 + ß0 + ß1x1 + ß2x2 + ß3x3 + ßx1x2 + ßx1x3 + ßx4x2 + ß x4x3 + ßx5x2 + ßx5x3 + e



- M4: y = ß0 + ß1x1 + ß2x2 + ß3x3 + ßx1x2 + ßx1x3 + ßx4x2 + ß x4x3 + ßx5x2 + ßx5x3 + e

This model tests if outrelief, the population ratio, and the old ratio effect pauperism, but are dependent on the year and area. 


4. What is the difference between *M3* and *M4*. What are the pros and cons of each parameterization?

- The intercept changes a little bit. The structure of the data remains the same, so the slopes should not change. All one does is move the intercept down slightly, which can help with interpreting the data. 

5. Conduct F-tests on the hypotheses: 
  
  1. All interactions in *M4* are 0

```{r}
library(car)
  
  linearHypothesis(M4, c("outratiodiff:year", "outratiodiff:TypeMixed", "outratiodiff:TypeUrban", "outratiodiff:TypeRural", "popratiodiff:year", "popratiodiff:TypeMixed", "popratiodiff:TypeUrban", "popratiodiff:TypeRural", "oldratiodiff:year", "oldratiodiff:TypeMixed", "oldratiodiff:TypeUrban", "oldratiodiff:TypeRural"))
```

 
  2. The coefficients on `outratiodiff` in *M4* are the same across years

```{r}
M4.1 <- M4
M4.2 <- lm(paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year), data = pauperism)
ftest1 <- anova(M4.1, M4.2)
ftest1

broom::tidy(ftest1)
broom::glance(ftest1)
```
  
  3. The coefficients on `outratiodiff` in *M4* are the same across PLU Types

```{r}
M4.3 <- lm(paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (Type), data = pauperism)

ftest2 <- anova(M4.1, M4.3)
ftest2

broom::tidy(ftest2)
broom::glance(ftest2)

```
  
  4. The coefficients on `outratiodiff` in *M4* are the same across PLU Types and years.

```{r}
#1 and 4 are the same
  
linearHypothesis(M4, c("outratiodiff:year", "outratiodiff:TypeMixed", "outratiodiff:TypeUrban", "outratiodiff:TypeRural", "popratiodiff:year", "popratiodiff:TypeMixed", "popratiodiff:TypeUrban", "popratiodiff:TypeRural", "oldratiodiff:year", "oldratiodiff:TypeMixed", "oldratiodiff:TypeUrban", "oldratiodiff:TypeRural"))


```

5. What is the predicted value of the median PLU in each year and PLU Type for these models. Include confidence intervals. Plot these as point-ranges with the estimate and confidence intervals.


```{r}
library("modelr")

pauperism <- mutate(pauperism, year = as.character(year))

plu_medians <- 
  pauperism %>%
  group_by(year, Type) %>%
  filter(!is.na(Type), year %in% c("1881", "1891")) %>%
  summarise_at(vars(outratiodiff, popratiodiff, oldratiodiff), 
               median, na.rm = TRUE)
mod4 <- lm(paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff), data = pauperism)

plu_predictions <- predict(mod4, newdata = plu_medians, interval = "confidence", level = 0.95)


as.data.frame(plu_predictions) %>% class()

plu_medians <- bind_cols(plu_medians, as.data.frame(plu_predictions))
plu_medians



#PLOT IT

ggplot(plu_medians, aes(Type, outratiodiff, color = year)) +
  geom_pointrange(aes(ymin = lwr, ymax = upr))
 

```

6. As previously, calculate the predicted value of the median PLU in each year and PLU Type. But instead of confidence intervals include the prediction interval. How do the confidence and prediction intervals differ? What are their definitions?

```{r}
plu_medians2 <- 
  pauperism %>%
  group_by(year, Type) %>%
  filter(!is.na(Type), year %in% c("1881", "1891")) %>%
  summarise_at(vars(outratiodiff, popratiodiff, oldratiodiff), 
               median, na.rm = TRUE)
mod4 <- lm(paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff), data = pauperism)

plu_predictions2 <- predict(mod4, newdata = plu_medians, interval = "predict")


as.data.frame(plu_predictions2) %>% class()

plu_medians2 <- bind_cols(plu_medians, as.data.frame(plu_predictions2))
plu_medians

```

The prediction interval is associated with the predicted value, whereas the confidence interval is associated with the fitted value.


## Functional Forms

The regression line of the model estimated in @Yule1899a (ignoring the year and region terms and interactions) can be also written as
$$
\begin{aligned}[t]
100 \times \frac{\mathtt{pauper2}_t / \mathtt{Popn2_t}}{\mathtt{pauper2}_{t-1} / \mathtt{Popn2_{t-1}}} 
&= \beta_0 + \beta_1 \times 100 \times \frac{\mathtt{outratio}_t}{\mathtt{outratio_{t-1}}} \\
& \quad + \beta_2 \times 100 \times \frac{\mathtt{Popn65}_t / \mathtt{Popn2}_{t}}{\mathtt{Popn65}_{t-1} / \mathtt{Popn2}_{t-1}} + \beta_3 \times 100 \times \frac{\mathtt{Popn2}_t}{\mathtt{Popn2}_{t - 1}}
\end{aligned}
$$

1. Take the logarithm of each side, and simplify so that $\log(\mathtt{pauper2}_t/\mathtt{pauper2}_{t -1})$ is the outcome and the predictors are all in the form $\log(x_t) - \log(x_{t - 1}) = \log(x_t / x_{t - 1})$.


```{r}
lm(log(pauper2) ~ log(outratio) + log(Popn2) + log(Prop65), data = pauperism)

```


2. Estimate the model with logged difference predictors, Year, and month and interpret the coefficient on $\log(outratio_t)$.

```{r}


```

3. What are the pros and cons of this parameterization of the model relative to the one in @Yule1899a? Focus on interpretation and the desired goal of the inference rather than the formal tests of the regression. Can you think of other, better functional forms?


- You shouldn't log, because there are negative values and you can't log negative values, hence you would bias your sample. 


## Non-differenced Model

Suppose you estimate the model (*M5*) without differencing,

```{r}
M5 <- lm(pauper2 ~ outratio + (Popn2 + Prop65) * (year + Type), data = pauperism)

M5

```

- Interpret the coefficient on `outratio`. How is this different than model *M2*

While M2 measures the raw chane, M5 measures the change in percentage.

- What accounts for the different in sample sizes in *M5* and *M2*?

oldratiodiff and popratiodiff has no values for 1871, because there is no previous year to compare to, hence no value.

- What model do you think will generally have less biased estimates of the effect of out-relief on pauperism: *M5* or *M2*? Explain your reasoning.

Model 2 will probably have a less-biased estimate of the effect of out-relief on pauperism since it is more differenced and thus is less valuable to omitted variable bias. 

## Substantive Effects

Read @Gross2014a and @McCaskeyRainey2015a. Use the methods described in those papers to assess the substantive effects of out-ratio on the rate of pauperism. Use the model(s) of your choosing.

Magnitude and Significance approach by McCaskey and Rainey

```{r}
M2 <- lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism)
summary(M2)

```

There is a statistically significant effect, as we can reject the null hypothesis.
The question now becomes wether this effect is not only statistically significant but also substantive, meaning if the magnitude of the effect has any actual effects or consequences on the outcome variable.The effect, albeit statistically significant, is fairly small at 0.23258. It should hence be questioned how substantive th effect is.



## Influential Observations and Outliers

### Influential Observations for the Regression

For this use *M2*:

1. For each observation, calculate and explain the following:

  - hat value (`hatvalues`)
  - standardized error (`rstandard`)
  - studentized error  (`rstudent`)
  - Cook's distance (`cooksd`)

```{r}
library(stats)

hatM2 <- hatvalues(M2)
std_hatM2 <- hatvalues(M2)/mean(hatvalues(M2))

rstdM2 <- rstandard(M2)

rstudM2 <- rstudent(M4)

cdM2 <- cooks.distance(M2)

```

The hat value measures the leverage of an outlier.

The standardized residual measures the discrepancy, or the size of the residual, of outliers and overcomes the problem of observations with high influence reducing their own residual.

The studentized residual measures the discrepancy of outliers as well, while overcoming the issue of the discrepancy of the observation itself reducing the residual.

Cook's Distance considers both the discrepancy and the leverage of outliers in one summary measure.

2. Create an outlier plot and label any outliers. See the example [here](https://jrnold.github.io/intro-methods-notes/outliers.html#iver-and-soskice-data)

```{r}
plot(std_hatM2, rstudM2, xlab = "Standardized hat-values", ylab = "Studentized Residual", main = "Influence Plot")
abline(h=c(-2,2), lty=2)
abline(v=c(2,3), lty=c(2,3))

```


3. Using the plot and rules of thumb identify outliers and influential observations

Any observation that is not within one of the two squares can be considered an outlier.


## Influential Observations for a Coefficient

1. Run *M2*, deleting each observation and saving the coefficient for `outratiodirff`. This is a method called the jackknife. You can use a for loop to do this, or you can use the function `jackknife` in the package [resamplr](https://github.com/jrnold/resamplr).

```{r}

jackknifeCOEF <- matrix(NA, ncol = 16, nrow = 1797 )
for (i in 1:nrow(pauperism)) {
  jackknifeCOEF[i, ] <- coef((lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism[-i, ])))
}


jackknifeCOEF
```

  
    - For which observations is there the largest change in the coefficient on `outratiodiff`?
    ```{r}
    apply(jackknifeCOEF, 2, min)
    apply(jackknifeCOEF, 2, max)
    
    ```
    
    The largest changes in coefficients on outratiofidd can be found within the observations of V6, V7, and V8, all shwowing differences of around 46,33      points
    
    1. Which observations have the largest effect on the estimate of `outratiodiff`? 
    
    The observations of V8 have the largest effect on the estimate of outratiodiff.
    
    2. How do these observations compare with those that had the largest effect on the overall regression as measured with Cook's distance?
    
    
    
    3. Compare the results of the jackknife to the `dfbeta` statistic for `outratiodiff`
    

```{r}
dfbeta(M2)

```



2. @AronowSamii2015a note that the influence of observations in a regression coefficient is different than the the influence of regression observations in the entire regression. Calculate the observation weights for `outratiodiff`.

    1. Regress `outratiodiff` on the control variables
        2. The weights of the observations are those with the highest squared errors from this regression. Which observations have the highest coefficient values? 
    3. How do the observations with the highest regression weights compare with those with the highest changes in the regression coefficient from the jackknife?

```{r}

reg.ord <- lm(outratiodiff ~ (popratiodiff + oldratiodiff) * (year + Type), data = pauperism)

sm <- summary(reg.ord)

sm.res <- sm$residuals^2
tail(sort(sm.res), 10)

```
The observations with the heighest coefficient values are observations 81, 242, 321, 456, 1947, 1098, 1134, 1185, 1620, and 1646.


## Omitted Variable Bias

An informal way to assess the potential impact of omitted variables on the coeficient of the variable of interest is to coefficient variation when covariates are added as a measure of the potential for omitted variable bias [@Oster2016a].
@NunnWantchekon2011a (Table 4) calculate a simple statistic for omitted variable bias in OLS. This statistic "provide[s] a measure to gauge the strength of the likely bias arising from unobservables: how much stronger selection on unobservables, relative to selection on observables, must be to explain away the full estimated effect."

1. Run a regression without any controls. Denote the coefficient on the variable of interest as $\hat\beta_R$.

```{r}
lm(paupratiodiff ~ outratiodiff, data = pauperism)

```

2. Run a regression with the full set of controls. Denote the coefficient on the variable of interest in this regression as $\hat\beta_F$. 

```{r}
lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism)

```



3. The ratio is $\hat\beta_F / (\hat\beta_R - \hat\beta_F)$

Calculate this statistic for *M2* and interpret it.

```{r}

ß.hat.r <- 0.3062
ß.hat.f <- 0.23258

ß.hat.f / (ß.hat.r - ß.hat.f)


```

Based on this ratio, the selection on unobservables must be 3.16 stronger, relative to the selection on observables, to explain away the full estimate effect.

## Heteroskedasticity

Robust Standard Errors

1. Run *M2* and *M3*  with a heteroskedasticity consistent (HAC), also called robust, standard error. How does this affect the standard errors on `outratio` coefficients? Use the **sandwich** package to add HAC standard errors [@Zeileis2004a].

```{r}
M2 <- lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism)
M3 <- lm(-1  + paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year + Type), data = pauperism)

#install.packages("sandwich")
library(sandwich)

sandwich::vcovHAC(M2)
sandwich::vcovHAC(M3)

summary(sandwich::vcovHAC(M2))
```

Multiple Regressions

1. Run the model with interactions for all years and types
2. For each subset of year and type run the regression.
    
```{r}
MReg1 <- lm(pauper2 ~ (outratio + Popn2 + Prop65) * year * Type - 1, data = pauperism)
    
MReg1

```
    
    2. How do the coefficients, standard errors, and regression standard errors ($\sigma$) differ from those of *M3*.
    
```{r}
all_interact <-
  crossing(Type = pauperism$Type, year = c(1881, 1891)) %>%
  mutate(mod = map2(year, Type, 
                    function(yr, ty) {
                    lm(paupratiodiff ~ outratiodiff + popratiodiff + oldratiodiff,
                       data = filter(pauperism,
                                      year == yr,
                                      Type == ty))
                    })) %>%
  mutate(mod_glance = map(mod, broom::glance),
         mod_tidy = map(mod, broom::tidy))


all_interact %>%
  mutate(sigma = map_dbl(mod_glance, function(x) x$sigma)) %>%
  select(year, Type, sigma)

```
    

## Weighted Regression

1. Run *M2* and *M3* as weighted regressions, weighted by the population (`Popn`) and interpret the coefficients on `outratiodiff` and interactions. Informally assess the extent to which the coefficients are different. Which one does it seem to affect more?

```{r}
M2.weighted <- lm(paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type), data = pauperism, weights = Popn)

M3.weighted <- lm(-1  + paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year + Type), data = pauperism, weights = Popn)

M2.weighted
M3.weighted

```
When weighted for Popn, a one unit increase in outratiodiff is associated on average with a 0.36447 increase in paupratiodiff for Model 2 and and a 0.71677 increase in Model 3.


2. What are some rationales for weighting by population? See the discussion in @SolonHaiderWooldridge2013a and @AngristPischke2014a.

To achieve precise estimates by correcting for heteroskedasticity, to achieve consistent estimates by correcting for endogenous smapling, and to identify average partial effects in the presence of unmodeled heterogeneity of effects.



## Cross-Validation

When using regression causal estimation, model specification and choice should largely be based on avoiding omitted variables. 
Another criteria for selecting models is to use their fit to the data.
But a model's fit to data should not be assessed using only the in-sample data.
That leads to overfitting---and the best model would always be to include an indicator variable for every observation
Instead, a model's fit to data can be assessed by using its out-of-sample fit.
One way to estimate the *expected* fit of a model to *new* data is cross-validation.

```{r}
mod_formulas <- 
  list(
    m0 = paupratiodiff ~ 1,
    m1 = paupratiodiff ~ year + Type,    
    m2 = paupratiodiff ~ outratiodiff + year + Type,
    m3 = paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * (year + Type),
    m4 = -1  + paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * (year + Type),
    m5 = paupratiodiff ~ (outratiodiff + popratiodiff + oldratiodiff) * year * Type
  )

pauperism_nonmiss <- 
  pauperism %>%
  filter(year %in% c(1881, 1891)) %>%
  select(paupratiodiff, outratiodiff, popratiodiff, oldratiodiff, year, Type, Region, ID) %>%
  tidyr::drop_na()
pauperism_10folds <-
  pauperism_nonmiss %>%
  resamplr::crossv_kfold(10)

mod_rmse_fold <- function(f, train, test) {
  fit <- lm(f, data = as.data.frame(train))
  test_data <- as.data.frame(test)
  err <- test_data$paupratiodiff - predict(fit, newdata = test_data)
  sqrt(mean(err ^ 2))
}

mod_rmse_fold(mod_formulas[[1]], pauperism_10folds$train[[1]],
              pauperism_10folds$test[[1]])

mod_rmse <- function(f, data) {
  map2_dbl(data$train, data$test, 
           function(train, test) {
             mod_rmse_fold(f, train, test)
           }) %>%
    mean()
}

mod_rmse(mod_formulas[[1]], pauperism_10folds)

cv_results <- tibble(
  model_formula = mod_formulas,
  .id = names(mod_formulas),
  # Formula as a string
  .name = map(model_formula,
              function(x) gsub(" +", " ", paste0(deparse(x), collapse = "")))
)

cv_results <-
  mutate(cv_results,
         cv10_rmse = map(model_formula, mod_rmse, data = pauperism_10folds))

loocv <- function(x) {
  mean((residuals(x) / (1 - hatvalues(x))) ^ 2)
}

cv_results <- 
  mutate(cv_results, 
         rmse_loo = map(mod_formulas, function(f) sqrt(loocv(lm(f, data = pauperism_nonmiss)))))

```

1. In the 10-fold cross validation, which model has the best out of sample prediction?

Since m0 has the highest value, it has the best out of sample prediction


2. Using the LOO-CV cross-validation, which model has the best

As with the 10-fold CV above, m0 has the highest score.


3. Does the prediction metric (RMSE) and prediction task---predicting individual PLUs from other PLUs---make sense? Can you think of others that you would prefer?



## Bootstrapping

Estimate the 95% confidence intervals of model with simple non-parametric bootstrapped standard errors. The non-parametric bootstrap works as follows:

Let $\hat\theta$ be the estimate of a statistic. To calculate bootstrapped standard errors and confidence intervals use the following procedure.

For samples $b = 1, ..., B$.

1. Draw a sample with replacement from the data
2. Estimate the statistic of interest and call it $\theta_b^*$.

Let $\theta^* = \{\theta_1^*, \dots, \theta_B^*\}$ be the set of bootstrapped statistics.

- standard error: $\hat\theta$ is $\sd(\theta^*)$.
- confidence interval:

    - normal approximation. This calculates the confidence interval as usual but uses the bootstrapped standard error instead of the classical OLS standard error: $\hat\theta \pm t_{\alpha/2,df} \cdot \sd(\theta^*)$
    - quantiles: A 95% confidence interval uses the 2.5% and 97.5% quantiles of $\theta^*$ for its upper and lower bounds.

```{r}
mod_formula <- paupratiodiff ~ outratiodiff + (popratiodiff + oldratiodiff) * year * Type
mod_orig <- lm(mod_formula, data = pauperism_nonmiss)

bs_coef_se <-
  resamplr::bootstrap(pauperism_nonmiss, 1024) %>%
  # extract the strap column
  `[[`("sample") %>%
  # run 
  map_df(function(dat) {
    lm(mod_formula, data = dat) %>%
    broom::tidy() %>%
    select(term, estimate)
  }) %>%
  # calculate 2.5%, 97.5% and sd of estimates
  group_by(term) %>%
  summarise(
    std.error_bs = sd(estimate),
    conf.low_bsq = quantile(estimate, 0.025),
    conf.low_bsq = quantile(estimate, 0.975)
  )

broom::tidy(mod_orig, conf.int = TRUE) %>%
  select(term, estimate, std.error) %>%
  filter(term == "outratiodiff") %>%
  left_join(bs_coef_se, by = "term")


sqrt(sandwich::vcovHC(mod_orig)["outratiodiff", "outratiodiff"])

pauperism_nonmiss %>%
  group_by(ID) %>%
  resamplr::bootstrap(1024) %>%
  # extract the strap column
  `[[`("sample") %>%
  # run 
  map_df(function(dat) {
    lm(mod_formula, data = dat) %>%
    broom::tidy() %>%
    select(term, estimate)
  }) %>%
  # calculate 2.5%, 97.5% and sd of estimates
  group_by(term) %>%
  summarise(
    std.error_bs = sd(estimate),
    conf.low_bsq = quantile(estimate, 0.025),
    conf.low_bsq = quantile(estimate, 0.975)
  ) %>%
  filter(term == "outratiodiff")

```

1. Try bootstrapping "Region" and "BoothGroup". Do either of these make much difference in the standard errors.

```{r}

broom::tidy(mod_orig, conf.int = TRUE) %>%
  select(term, estimate, std.error) %>%
  filter(term == "Region") %>%
  left_join(bs_coef_se, by = "term")


#sqrt(sandwich::vcovHC(mod_orig)["Region", "Region"])

pauperism_nonmiss %>%
  group_by(ID) %>%
  resamplr::bootstrap(1024) %>%
  # extract the strap column
  `[[`("sample") %>%
  # run 
  map_df(function(dat) {
    lm(mod_formula, data = dat) %>%
    broom::tidy() %>%
    select(term, estimate)
  }) %>%
  # calculate 2.5%, 97.5% and sd of estimates
  group_by(term) %>%
  summarise(
    std.error_bs = sd(estimate),
    conf.low_bsq = quantile(estimate, 0.025),
    conf.low_bsq = quantile(estimate, 0.975)
  ) %>%
  filter(term == "Region")

```


