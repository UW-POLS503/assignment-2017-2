---
title: "POLS 503: HW 2"
---

```{r}
library("dplyr")
library("tidyr")
library("tidyverse")
library("modelr")
library("viridis")
library("broom")
library("ggplot2")
library("modelr")
library("viridis")
```

## The Effect of English Poor Laws on Pauperism

[Yule (1899)](https://www.jstor.org/stable/2979889?seq=1#page_scan_tab_contents) is a published example multiple regression analysis in its modern form.[^yule]

Yule wrote this paper to analyze the effect of policy changes and implementation on pauperism (poor receiving benefits) in England under the [English Poor Laws](https://en.wikipedia.org/wiki/English_Poor_Laws). In 1834, a new poor law was passed that established a national welfare system in England and Wales. The New Poor Law created new administrative districts (Poor Law Unions) to adminster the law. Most importantly, it attempted to standardize the provision of aid to the poor. There were two types of aid provided: in-relief or aid provided to paupers in workhouses where they resided, and out-relief or aid provided to paupers residing at home. The New Poor Law wanted to decrease out-relief and increase in-relief in the belief that in-relief, in particular the quality of life in workhouses, was a deterrence to poverty and an encouragement for the poor to work harder to avoid poverty.

Yule identifies that there are various potential causes of the change in rate of pauperism, including changes in the (1) law, (2) economic conditions, (3) general social character, (4) moral character, (5) age distribution of the population (pg. 250).

He astutely notes the following:

> If, for example, we should find an increase in the proportion of out-relief associated with (1) an increase in the proportion of the aged to the whole population, and also (2) an increase in the rate of pauperism, it might be legitimate to interpret the result in the sense that changes in out-relief and pauperism were merely simultaneous concomitants of changes in the proportion of aged-the change of pauperism not being a direct consequence of the change of administration, but both direct consequenices of the change in age distribution. It is evidently most important that we should be able to decide between two such differenit ilnterpretations of the same facts. This the method I have used is perfectly competernt to do  - Yule (1899, pg. 250)

[^yule]: See Stigler(2016) and Stigler(1990) for a discussion.

### Data

The Yule's data on pauperism is included in the package datums at `jrnold/datums`
```{r}
# devtools::install_github("jrnold/datums")
library(datums)
```
It consists of two datasets: pauperism_plu contains data on the Poor Law Unions, and pauperism_year has the PLU-year as the unit of observation and contains data on the levels of pauperism in 1871, 1881, and 1891 in each PLU.
```{r}
pauperism_plu <- datums::pauperism_plu
pauperism_year <- datums::pauperism_year
glimpse(pauperism_year)
```
There are four variables of primary interest to Yule (pg. 252-254):

**Pauperism**--the percentage of the population in receipt of relief of any kind, less lunatics and vagrants;

**Out-Relief Ratio**--the ratio of numbers relieved outdoors to those relieved indoors;

**Proportion of Old**--the proportion of the aged (65 years) to the whole population;

**Population**--used to capture economic, social, or moral factors.

There is also **Grouping of Unions**, which is a locational classification based on population density that consists of Rural, Mixed, Urban, and Metropolitan.

Instead of taking differences or percentages, Yule worked with "percent ratio differences", $100 \times \frac{x_{t}}{x_{t-1}}$, because he did not want to work with negative signs, presumably a concern at the because he was doing arithmetic by hand and this would make calculations more tedious or error-prone.

We can construct the **Proportion of Old** variable below since it is not included in the datasets.

The `pctratiodiff` function computes percentage ratio differences:
```{r}
pctratiodiff <- function(x) {  
  z <- 100 * (x / lag(x))
  # if lag(x) == 0 then z is missing
  z[!is.finite(z)] <- NA_real_
  z
}

# adjust the dataset by creating Popn65 and Prop65 variables
pauperism <-  
  pauperism_year %>%
  mutate(pauper = coalesce(pauper2, pauper),
         Popn = coalesce(Popn2, Popn),
         # total of men and women over 65
         Popn65 = F65 + M65,
         # proportion of total population 65 and above
         Prop65 = Popn65 / Popn,
         year = as.integer(year)) %>%
  arrange(ID, year) %>%
  group_by(ID) %>%
  mutate(Prop65_diff = pctratiodiff(Prop65)) %>%
  # for each PLU, create Prop65_diff using the above function
  left_join(pauperism_plu, by = "ID") %>%
  ungroup()
```

## Models

Consider the Yule 1899 data on the relationship between pauperism and the way in which welfare was 
This data was re-constructed by [Plewis (2017)](http://onlinelibrary.wiley.com/doi/10.1111/rssa.12272/full) and is distributed with the **datums** package available on github,
```{r}
# devtools::install_github("jrnold/datums")
```

## Regression Models

Run regressions of `pauper` using the yearly level data with the following specifications:

1. `outratio` only
2. `outratio` plus controls
3. `outratio` plus controls, year dummies, and region dummies.
4. `outratio` with controls and interactions with year and region dummies

Now run the same set of four specifications but using the ratio difference variables: `outratiodiff`, `paupratiodiff`, ...


Read the R for data science section [Many Models](http://r4ds.had.co.nz/many-models.html) for tips on how to estimate it.

## Functional Form

- log both sides, simplify terms and interpret
- use the functional form in the Plewis paper

Can you think of other functional forms.

## Influential Observations and Outliers

### Influential Observations

1. For each observation, calculate and explain the following:

  - hat value (`hatvalues`)
  - standardized error (`rstandard`)
  - studentized error  (`rstudent`)
  - Cook's distance (`cooksd`)

2. Produce a plot
3. Using the plot and rules of thumb identify outliers and influential observations.

## Influential Observations for outratio

1. Run the regression dropping each observation and saving the coefficient for `outratio`. This is a method called a `jackknife`.
  
    - For which observations is there the largest change?
    - Which observations have the largest effect on our estimates of `outratio`? 
    - How do these observations compare with those that had the largest effect on the 

2. [Aronow and Samii (2016)](http://onlinelibrary.wiley.com/doi/10.1111/ajps.12185/abstract) note that the influence of observations in a regression depends on which observations 


## Omitted Variable Bias

An informal way to assess the potential impact of omitted variables on the coeficient of the variable of interest is to coefficient variation when covariates are added as a measure of 
[Nunn and Wantchekon](https://scholar.harvard.edu/files/nunn/files/nunn_wantchekon_aer_2011.pdf) (Table 4) calculate a simple statistic for omitted variable bias in OLS. This statistic "provide[s] a measure to gauge the strength of the likely
bias arising from unobservables: how much stronger selection on unobservables,
relative to selection on observables, must be to explain away the full estimated
effect."

1. Run a regression without any controls. Denote the coefficient on the variable of interest as $\hat\beta_R$.
2. Run a regression with the full set of controls. Denote the coefficient on the variable of interest in this regression as $\hat\beta_F$. 
3. The ratio is $\hat\beta_F / (\hat\beta_R - \hat\beta_F)$

**Q** Calculate this statistic for each regression and interpret it.


## Heteroskedasticity

Run the model with robust standard errors that asymtotically correct for heteroskedasticity.
How do your inferences change?


## Weighted Regression

We may be interested on the effect weighted by population. Run a regression weighted by Popn and interpret. Why would it make sense to weight by population? 

## Average Marginal Effects

**TODO**

## Cross-Validation

Compare the previous models you have estimated using cross-validation

1. OLS LOO-CV value
2. 5-fold CV with RMSE

The previous cross-validation methods had two assumptions: error metric and the observations w

## Bootstrapping

Estimate the 95% confidence intervals of model with
simple non-parametric bootstrapped standard errors. 
The non-parametric bootstrap works as follows:

Let $\hat\theta$ be the estimate of a statistic. To calculate bootstrapped standard errors and confidence intervals use the following procedure.

For samples $b = 1, ..., B$.

1. Draw a sample with replacement from the data
2. Estimate the statistic of interest and call it $\theta_b^*$.

Let $\theta^* = \{\theta_1^*, \dots, \theta_B^*}$ be the set of bootstrapped statistics.

- standard error: $\hat\theta$ is $sd(\theta^*)$.
- confidence interval:

    - normal approximation. This calculates the confidence interval as usual but uses the bootstrapped standard error instead of the classical OLS standard error: $\hat\theta \pm t \sd(\theta^*)$
    - quantiles: A 95% confidence interval uses the 2.5% and 97.5% quantiles of $\theta^*$ for its upper and lower bounds.
